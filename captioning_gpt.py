# -*- coding: utf-8 -*-
"""captioning_gpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oOG2fKkPXcCcZq5BPtXRzhNw47IfVhJ1
"""

!pip install streamlit

import streamlit as st
import zipfile
import numpy as np
import pandas as pd
import os

# import Deep learning Libraries
import tensorflow as tf
from tqdm import tqdm
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import Sequence
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer, BatchNormalization
from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional
from tensorflow.keras.applications import VGG16, ResNet50, DenseNet201, EfficientNetB3
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, Adamax
from keras.applications import VGG16

import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from textwrap import wrap

plt.rcParams['font.size'] = 12
sns.set_style("dark")
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

import zipfile
!unzip '/content/drive/MyDrive/ulkus_diabetes.zip'

image_path = '/content/Patches/images'

data = pd.read_csv("/content/Patches/diabetes_ulcer_final_varied_dataset.csv")
data.head()

# loading the dataset
def loading_the_data(data_dir):
    # Generate data paths with labels
    filepaths = []
    labels = []

    # Get folder names
    folds = os.listdir(data_dir)

    for fold in folds:
        foldpath = os.path.join(data_dir, fold)
        filelist = os.listdir(foldpath)
        for file in filelist:
            fpath = os.path.join(foldpath, file)

            filepaths.append(fpath)
            labels.append(fold)

    # Concatenate data paths with labels into one DataFrame
    Fseries = pd.Series(filepaths, name='filepaths')
    Lseries = pd.Series(labels, name='labels')

    df = pd.concat([Fseries, Lseries], axis=1)

    return df

# loading the data
data_dir = '/content/Patches/folder'
df = loading_the_data(data_dir)

df

"""## **Visualization**"""

data_balance = df.labels.value_counts()


def custom_autopct(pct):
    total = sum(data_balance)
    val = int(round(pct*total/100.0))
    return "{:.1f}%\n({:d})".format(pct, val)


# pie chart for data balance
plt.pie(data_balance, labels = data_balance.index, autopct=custom_autopct, colors = ["#57A6DE","#5D57DE","#577BDE","#43CFE0","#A0B1DE"])
plt.title("Data balance")
plt.axis("equal")
plt.show()

# data --> 80% train data && 20% (test, val)
train_df, ts_df = train_test_split(df, train_size = 0.8, shuffle = True, random_state = 42)

# test data --> 10% train data && 10% (test, val)
valid_df, test_df = train_test_split(ts_df, train_size = 0.5, shuffle = True, random_state = 42)

batch_size = 16
img_size = (224, 224)

# Create generators for the data
tr_gen = ImageDataGenerator(rescale=1. / 255)
ts_gen = ImageDataGenerator(rescale=1. / 255)


# Convert the data
train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)

valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= True, batch_size= batch_size)

test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                    color_mode= 'rgb', shuffle= False, batch_size= batch_size)

g_dict = train_gen.class_indices      # defines dictionary {'class': index}
classes = list(g_dict.keys())       # defines list of dictionary's kays (classes), classes names : string
images, labels = next(train_gen)      # get a batch size samples from the generator

# ploting the patch size samples
plt.figure(figsize= (20, 20))

for i in range(batch_size):
    plt.subplot(4, 4, i + 1)
    image = images[i]
    plt.imshow(image)
    index = np.argmax(labels[i])  # get image index
    class_name = classes[index]   # get class of image
    plt.title(class_name, color= 'black', fontsize= 16)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Displaying the model performance
def model_performance(history, Epochs):
    # Define needed variables
    tr_acc = history.history['accuracy']
    tr_loss = history.history['loss']
    val_acc = history.history['val_accuracy']
    val_loss = history.history['val_loss']

    Epochs = [i+1 for i in range(len(tr_acc))]

    # Plot training history
    plt.figure(figsize= (20, 8))
    plt.style.use('fivethirtyeight')

    plt.subplot(1, 2, 1)
    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.tight_layout
    plt.show()

# Evaluate the model
def model_evaluation(model):
    train_score = model.evaluate(train_gen, verbose= 1)
    valid_score = model.evaluate(valid_gen, verbose= 1)
    test_score = model.evaluate(test_gen, verbose= 1)

    print("Train Loss: ", train_score[0])
    print("Train Accuracy: ", train_score[1])
    print('-' * 20)
    print("Validation Loss: ", valid_score[0])
    print("Validation Accuracy: ", valid_score[1])
    print('-' * 20)
    print("Test Loss: ", test_score[0])
    print("Test Accuracy: ", test_score[1])

# Get Predictions
def get_pred(model, test_gen):

    preds = model.predict(test_gen)
    y_pred = np.argmax(preds, axis = 1)

    return y_pred

# Confusion Matrix
def plot_confusion_matrix(test_gen, y_pred):

    g_dict = test_gen.class_indices
    classes = list(g_dict.keys())

    # Display the confusion matrix
    cm = confusion_matrix(test_gen.classes, y_pred)

    plt.figure(figsize= (10, 10))
    plt.imshow(cm, interpolation= 'nearest', cmap= plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation= 45)
    plt.yticks(tick_marks, classes)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')

    plt.tight_layout()
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')

    plt.show()

# Defining a convolutional NN block for a sequential CNN model
def conv_block(filters, act='relu'):

    block = Sequential()
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(BatchNormalization())
    block.add(MaxPooling2D())

    return block

# Defining a dense NN block for a sequential CNN model
def dense_block(units, dropout_rate, act='relu'):

    block = Sequential()
    block.add(Dense(units, activation=act))
    block.add(BatchNormalization())
    block.add(Dropout(dropout_rate))

    return block

# create Model structure
img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)

class_counts = len(list(train_gen.class_indices.keys()))     # to define number of classes in dense layer

"""## **VGG16**"""

# get the pre-trained model (VGG16)
base_model = VGG16(weights='imagenet', include_top=False, input_shape = img_shape, pooling= 'max')

# freeze four convolution blocks
for layer in base_model.layers[:15]:
    layer.trainable = False


# fine-tune VGG16 (Adding some custom layers on top)
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation = 'relu')(x)
x = Dropout(0.2)(x)   # # Dropout layer to prevent overfitting
x = Dense(256, activation = 'relu')(x)
x = Dense(128, activation = 'relu')(x)
x = Dense(32, activation = 'relu')(x)
predictions = Dense(class_counts, activation = "sigmoid")(x)    # output layer with softmax activation

# the model
VGG16_model = Model(inputs = base_model.input, outputs = predictions)

# Check the trainable status of the layers
for layer in VGG16_model.layers:
    print(layer.name, layer.trainable)

VGG16_model.compile(optimizer=Adamax(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])

VGG16_model.summary()

# train the model
epochs = 50   # number of all epochs in training

VGG16_history = VGG16_model.fit(train_gen, epochs= epochs, verbose= 1, validation_data= valid_gen, shuffle= False)

# Display model performance
model_performance(VGG16_history, epochs)

# Model evaluation
model_evaluation(VGG16_model)

import matplotlib.pyplot as plt

# Extract history data
vgg16_loss = VGG16_history.history['loss']
vgg16_val_loss = VGG16_history.history['val_loss']
vgg16_accuracy = VGG16_history.history['accuracy']
vgg16_val_accuracy = VGG16_history.history['val_accuracy']

# Create subplots for loss
plt.figure(figsize=(12, 6))

# Plot loss
plt.subplot(1, 2, 1)
plt.plot(vgg16_loss, label='VGG16 Training Loss')
plt.plot(vgg16_val_loss, label='VGG16 Validation Loss')
plt.title('Loss Comparison')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(vgg16_accuracy, label='VGG16 Training Accuracy')
plt.plot(vgg16_val_accuracy, label='VGG16 Validation Accuracy')
plt.title('Accuracy Comparison')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Create a dictionary to hold model histories
model_dict = {'VGG16': VGG16_history}

# Prepare a DataFrame to store metrics
metrics = []

# Extract training and validation metrics for each model
for model_name, model_hist in model_dict.items():
    final_train_accuracy = model_hist.history['accuracy'][-1]
    final_val_accuracy = model_hist.history['val_accuracy'][-1]
    final_train_loss = model_hist.history['loss'][-1]
    final_val_loss = model_hist.history['val_loss'][-1]

    metrics.append({
        'Model': model_name,
        'Final Train Accuracy': final_train_accuracy,
        'Final Val Accuracy': final_val_accuracy,
        'Final Train Loss': final_train_loss,
        'Final Val Loss': final_val_loss
    })

# Create a DataFrame from the metrics
df_metrics = pd.DataFrame(metrics)

# Display the metrics DataFrame
print(df_metrics)

# Optionally, you can plot the final validation accuracy
fig, ax = plt.subplots()

# Set index for horizontal bar plot
df_metrics.set_index('Model')[['Final Train Accuracy', 'Final Val Accuracy']].plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)
ax.set_title('Final Training and Validation Accuracy')
ax.set_xlabel('Accuracy')
ax.set_ylabel('Model')

plt.tight_layout()
plt.show()

"""## **Test menggunakan VGG16**"""

# Load gambar baru untuk pengujian
image_path = '/content/Patches/images/normal (403).jpg'
# Change target_size to (224, 224) to match VGG16 input requirements
img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = tf.expand_dims(img_array, axis=0)  # Menambahkan batch dimension

# Normalisasi gambar jika diperlukan (sesuai dengan preprocessing pada data training)
img_array = img_array / 255.0

# Assuming 'VGG16_model' or 'cnn_model' is your trained model
# Replace 'your_model_variable' with the actual name of your model variable
prediction = VGG16_model.predict(img_array) # Changed 'history' to 'VGG16_model' -  You should use the actual variable name of your trained model

# Interpretasi hasil prediksi
if prediction[0][0] > 0.5:
    label = "Abnormal"
else:
    label = "Normal"

# Tampilkan gambar dan hasil prediksi
plt.imshow(img)
plt.title(f"Prediction: {label}")
plt.axis('off')
plt.show()

import openai

# Step 1: Set up OpenAI API Key
openai.api_key = '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'

# Step 2: Load Pre-trained VGG16 Model
# Using VGG16 pre-trained on ImageNet for feature extraction
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the convolutional base
for layer in base_model.layers:
    layer.trainable = False

# Adding custom layers on top of VGG16 for classification
x = base_model.output
x = Flatten()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(2, activation='softmax')(x)  # Assuming binary classification (Normal/Abnormal)

# Final model
model_vgg = Model(inputs=base_model.input, outputs=predictions)
model_vgg.compile(optimizer=Adamax(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])

model_vgg.summary()

def extract_features(image_path):
    """
    Function to extract features from an image using VGG16.
    Args:
        image_path (str): Path to the input image.
    Returns:
        np.array: Extracted feature vector.
    """
    # Load and preprocess the image for VGG16
    img = load_img(image_path, target_size=(224, 224))
    img_array = img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)

    # Extract features using VGG16
    features = base_model.predict(img_array)
    return features

def generate_caption(features):
    """
    Generate a caption for an image using ChatGPT.
    Args:
        features (np.array): Feature vector extracted from the image.
    Returns:
        str: Generated caption for the image.
    """
    # Generate a descriptive prompt to ask ChatGPT for diabetic ulcer analysis
    prompt = (
        "Berdasarkan fitur gambar ulkus diabetes yang diberikan, berikan analisis berikut:"
        "1. Level keparahan ulkus: deskripsikan tingkat keparahan ulkus (misalnya, ringan, sedang, parah)."
        "2. Infeksi: apakah ada indikasi infeksi pada ulkus? Jelaskan dengan detail."
        "3. Rekomendasi perawatan: berikan saran untuk perawatan ulkus tersebut."
        "4. Caption gambar: buat deskripsi singkat tentang gambar ini.")
    response = openai.Completion.create(
        engine="text-davinci-003",
        prompt=prompt,
        max_tokens=150
    )
    caption = response.choices[0].text.strip()
    return caption

# Step 3: Streamlit Web Application
# Set up the Streamlit interface
st.title("Diabetic Ulcer Image Analysis using VGG16 and ChatGPT")
st.write("Upload an image of a diabetic ulcer, and this app will generate a detailed analysis including the level of severity, infection status, treatment recommendation, and a descriptive caption.")

# File uploader for image input
uploaded_file = st.file_uploader("Choose an image of a diabetic ulcer...", type=["jpg", "jpeg", "png"])

# Process the uploaded image if available
if uploaded_file is not None:
    # Save the uploaded file to a temporary directory
    temp_dir = "tempDir"
    if not os.path.exists(temp_dir):
        os.makedirs(temp_dir)
    image_path = os.path.join(temp_dir, uploaded_file.name)
    with open(image_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    # Display the uploaded image
    st.image(uploaded_file, caption='Uploaded Image', use_column_width=True)

    # Extract features from the uploaded image using VGG16
    st.write("Extracting features from the image...")
    features = extract_features(image_path)

    # Generate a caption and analysis for the uploaded image using ChatGPT
    st.write("Generating analysis and caption...")
    caption = generate_caption(features)

    # Display the generated caption and analysis
    st.write("Generated Analysis and Caption:")
    st.success(caption)

# To run this app, use the command: streamlit run app.py

pip freeze > requirements.txt

from tensorflow.keras.models import save_model

save_model(model_vgg, 'model_vgg.h5')

